{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mokapot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path\n",
    "import data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this function is to clean up the original 'before' data so that we are not counting decoys or duplicate scans.\n",
    "Mokapot needs the decoys, so they are only removed from the 'before' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, prob_column='PeptideProphet Probability'):\n",
    "     #drop decoy\n",
    "    df = df[df[\"decoy\"]==False]\n",
    "    #sort by qvalue\n",
    "    df = df.sort_values(prob_column)\n",
    "    #drop duplicate scans\n",
    "    df = df.drop_duplicates(subset=[\"scan\"], keep=\"first\") #keep highest scoring\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MsFragger does not have a q value, instead it has a \"PeptideProphet Probability\". Their scoring of this works backwards from all the other tools. A number closer to 1 is better than a number closer to 0. In order to compare the tools all together, we convert this score to the same scale as all the rest of our data. This function accomplishes that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chaning the probability column to be the same scale as all the other tools\n",
    "def set_probablility(row):\n",
    "    new_prob = 1 - row['PeptideProphet Probability']\n",
    "    return new_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MokaPot needs a column that notates where a scan was decoy or not. The \"make_target_col_msfragger\" function generates that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target_col_msfragger(row):\n",
    "    if row[\"Protein\"].startswith(\"rev\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MsFragger combines the scan number with the file name.\n",
    "In order to compares MsFragger to the rest of the tools we \n",
    "use the \"extractScanNum\" function to seperate the scan number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling only scan numbers out\n",
    "def extractScanNum(row):\n",
    "    string = row\n",
    "    spot = string.find('.')\n",
    "    new_st = string[spot + 1:]\n",
    "    spot = new_st.find('.')\n",
    "    final_st = new_st[:spot]\n",
    "    \n",
    "    if final_st[0] == \"0\":\n",
    "        final_st = final_st[1:]\n",
    "    return final_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us back the \"before\" dataset. We will clean the data by dropping duplicates and decoys. The purpose of this is to be able to count how many scans we originally have at or under a specific cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PreMokaPot_data(file):\n",
    "    msf_df = data_loader.clean_msfragger(file)\n",
    "    msf_df = filter_data(msf_df)\n",
    "    \n",
    "    #Changing the probabilities to the same scale all the other tools use \n",
    "    msf_df[\"Updated_probability\"] = msf_df.apply(set_probablility, 1)\n",
    "    return msf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we will get the data, and format it for Mokapot. \n",
    "We have to make the target column to represent decoys, we rename the scan column to match the rest of the data when we make the megascript, and change the probabililty column to the same scale as the output from the rest of the tools. We also drop any duplicate scan numbers before we feed the data into MokaPot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_MokaPot(file):\n",
    "    msf_df = data_loader.clean_msfragger(file)\n",
    "    \n",
    "    msf_df[\"target_column\"] = msf_df.apply(make_target_col_msfragger, axis = 1)\n",
    "    \n",
    "     #Extracting scan number from file number\n",
    "    msf_df['scan'] =msf_df['scan'].apply(extractScanNum) \n",
    "    \n",
    "    msf_df = msf_df.rename(columns = {\"scan\": \"ScanNr\"})\n",
    "    \n",
    "    #Changing the probabilities to the same scale all the other tools use. Dropping the old probability column\n",
    "    msf_df[\"Updated_probability\"] = msf_df.apply(set_probablility, 1)\n",
    "    \n",
    "    #sort by q value and drop duplicate scans\n",
    "    msf_df = msf_df.sort_values(\"Updated_probability\")\n",
    "    msf_df = msf_df.drop_duplicates(subset=[\"ScanNr\"], keep=\"first\") #keep highest scoring\n",
    "    \n",
    "    return msf_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading all the files into a list here\n",
    "file_names = [\"2ng_rep1\", \"2ng_rep2\", \"2ng_rep3\", \"2ng_rep4\", \"2ng_rep5\", \"2ng_rep6\",\n",
    "             \"0.2ng_rep1\", \"0.2ng_rep2\", \"0.2ng_rep3\", \"0.2ng_rep4\", \"0.2ng_rep5\", \"0.2ng_rep6\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we run each input file through MokaPot. Each file we read in is analyzed and the results are saved in a seperate output file. \n",
    "\n",
    "MokaPot requires feature columns that it uses to give each scan a new q value score. The columns from MsFragger that we have chosen to use as features are: 'Peptide Length', 'Retention', 'Delta Mass', 'Expectation', 'Hyperscore', 'Nextscore', 'Number of Enzymatic Termini', 'Number of Missed Cleavages', 'Intensity'. 'Charge' is also used as a one hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 2ng_rep1:\n",
      "\tMsFragger: 10956\n",
      "\tMsFragger and MokaPot: 13152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 2ng_rep2:\n",
      "\tMsFragger: 10808\n",
      "\tMsFragger and MokaPot: 12710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 2ng_rep3:\n",
      "\tMsFragger: 8324\n",
      "\tMsFragger and MokaPot: 10565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of PSMs found at or above 0.01 for 2ng_rep4:\n",
      "\tMsFragger: 8787\n",
      "\tMsFragger and MokaPot: 10409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 2ng_rep5:\n",
      "\tMsFragger: 13448\n",
      "\tMsFragger and MokaPot: 15195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 2ng_rep6:\n",
      "\tMsFragger: 12538\n",
      "\tMsFragger and MokaPot: 14189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 0.2ng_rep1:\n",
      "\tMsFragger: 5693\n",
      "\tMsFragger and MokaPot: 6730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 0.2ng_rep2:\n",
      "\tMsFragger: 5601\n",
      "\tMsFragger and MokaPot: 6743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 0.2ng_rep3:\n",
      "\tMsFragger: 4264\n",
      "\tMsFragger and MokaPot: 5185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 0.2ng_rep4:\n",
      "\tMsFragger: 3918\n",
      "\tMsFragger and MokaPot: 4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 0.2ng_rep5:\n",
      "\tMsFragger: 4564\n",
      "\tMsFragger and MokaPot: 5392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Learned model did not improve over the best feature. Now scoring by the best feature for each collection of PSMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: IRLS did not converge with maxIter = 50\n",
      "Warning: IRLS did not converge with maxIter = 50\n",
      "The number of PSMs found at or above 0.01 for 0.2ng_rep6:\n",
      "\tMsFragger: 4302\n",
      "\tMsFragger and MokaPot: 5110\n"
     ]
    }
   ],
   "source": [
    "for file in file_names:\n",
    "    msf_cleaned_df = get_PreMokaPot_data(file)\n",
    "    msf_df = get_data_for_MokaPot(file)\n",
    "    \n",
    "    charge_feat = pd.get_dummies(msf_df[\"Charge\"], prefix=\"Charge\")\n",
    "    msf_df = pd.concat([msf_df, charge_feat], axis=1)\n",
    "    msf_for_MP = mokapot.dataset.LinearPsmDataset(msf_df, target_column = \"target_column\", spectrum_columns = \"ScanNr\", \n",
    "                                                  peptide_column = \"peptide\", protein_column=None, \n",
    "                                                  group_column=None, feature_columns= (list(charge_feat.columns) +  ['Peptide Length', 'Retention', 'Delta Mass', \n",
    "                                                  'Expectation', 'Hyperscore', 'Nextscore', 'Number of Enzymatic Termini', \n",
    "                                                  'Number of Missed Cleavages', 'Intensity']), copy_data=True)\n",
    "\n",
    "    results, models = mokapot.brew(msf_for_MP)\n",
    "\n",
    "    results_df = results.psms\n",
    "    results_df.to_csv(\"MokaPot_Output/MsFragger/msf_\" + file + \".csv\")\n",
    "    \n",
    "    print(\"The number of PSMs found at or above 0.01 for \" + file + \":\")   \n",
    "    print(\"\\t\" + \"MsFragger: \" + str(len(msf_df[msf_df['Updated_probability'] <= 0.01])))\n",
    "    print(\"\\t\"\"MsFragger and MokaPot: \" + str(len(results.psms[results.psms['mokapot q-value'] <= 0.01])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
