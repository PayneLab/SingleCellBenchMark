{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'megatable_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f18bd43b6d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmegatable_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'megatable_loader'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mokapot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path\n",
    "import data_loader as dl\n",
    "import megatable_loader as ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our Figure with Your Data - a tutorial\n",
    "\n",
    "This notebook describes how you can compare data from an algorithm to output for algorithms that have been previously benchmarked. \n",
    "\n",
    "In order to compare your program's output to our benchmarked data, you will need to run the same raw files  through your algorithm. Begin by downloading six 2ng and six 0.2ng files from PRIDE (URL?). Run these through your program and save the files. \n",
    "\n",
    "To accuratly run your files against ours, each file will need to have a \"scan\", \"decoy\", \"peptide\", and some type of probability column.\n",
    "- \"scan\" is the scan number column . \n",
    "- \"decoy\" is a boolean column that tracks whether a scan was tagged as a decoy or not. True denotes a decoy, others wise it will be false.\n",
    "- \"peptide\" is the column that has a string with the peptide found for each scan\n",
    "- The probability column that presents the score/confidence that the tool gave to a specific scan. If your tool has a  qvalue or PEP value column, use this. All of the benchmarked data's probability column are scored on a scale where 0 is the best and 1 is the worst. You will want your data's probability column to be scaled this way as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Defining Your Variables\n",
    "\n",
    "The first thing that you have to do is to tell us where your data lives, and then we can get about parsing it and working to make the figure. Once you have ran the raw files and saved the correctly formatted output files, insert the file paths into the correct spot in the input_files list below. Make sure that the right file is loaded into the right spot or your tool's output will not be correctly compared to the benchmarked data.\n",
    "\n",
    "We will also ask you to type in the name of your probability column and the name of your tool.\n",
    "\n",
    "Afterwards, we will take your data paths and put them into a list. This will make it easier for us to parse your data and make sure each file is compared to the right benchmarked files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#please fill in the actual file paths to each of these output files on your computer. Each of these files are\n",
    "#the names of the raw files that you would have ran through your tool.\n",
    "\n",
    "Ex_Auto_J3_30umTB_2ngQC_60m_1 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_J3_30umTB_2ngQC_60m_2 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_K13_30umTA_2ngQC_60m_1 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_K13_30umTA_2ngQC_60m_2 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_W17_30umTA_2ngQC_60m_3 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_W17_30umTA_2ngQC_60m_4 = \"/path/to/your/file.txt\"\n",
    "\n",
    "Ex_Auto_J3_30umTB_02ngQC_60m_1 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_J3_30umTB_02ngQC_60m_2 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_K13_30umTA_02ngQC_60m_1 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_K13_30umTA_02ngQC_60m_2 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_W17_30umTA_02ngQC_60m_3 = \"/path/to/your/file.txt\"\n",
    "Ex_Auto_W17_30umTA_02ngQC_60m_4 = \"/path/to/your/file.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type in the name of your probability column and your tool. Afterwards, run the cell. \n",
    "\n",
    "#probability_column = \"put your probabiliy column name here\" \n",
    "#tool_name = \"type the name of your tool\"\n",
    "\n",
    "probability_column = \"probability\" #delete\n",
    "tool_name = \"New_Tool\" #Delete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are simply storing your file paths into a dictionary. This will help us to make sure we compare\n",
    "#your data to the correctly benchmarked files. There is nothing you need to edit here. Simply run the cell.\n",
    "\n",
    "input_files = {} \n",
    "    #2ng files\n",
    "input_files[\"2ng_rep1\"] = Ex_Auto_J3_30umTB_2ngQC_60m_1\n",
    "input_files[\"2ng_rep2\"] = Ex_Auto_J3_30umTB_2ngQC_60m_2\n",
    "input_files[\"2ng_rep3\"] = Ex_Auto_K13_30umTA_2ngQC_60m_1\n",
    "input_files[\"2ng_rep4\"] = Ex_Auto_K13_30umTA_2ngQC_60m_2\n",
    "input_files[\"2ng_rep5\"] = Ex_Auto_W17_30umTA_2ngQC_60m_3\n",
    "input_files[\"2ng_rep6\"] = Ex_Auto_W17_30umTA_2ngQC_60m_4\n",
    "\n",
    "    #0.2ng files\n",
    "input_files[\"0.2ng_rep1\"] = Ex_Auto_J3_30umTB_02ngQC_60m_1\n",
    "input_files[\"0.2ng_rep2\"] = Ex_Auto_J3_30umTB_02ngQC_60m_2\n",
    "input_files[\"0.2ng_rep3\"] = Ex_Auto_K13_30umTA_02ngQC_60m_1\n",
    "input_files[\"0.2ng_rep4\"] = Ex_Auto_K13_30umTA_02ngQC_60m_2\n",
    "input_files[\"0.2ng_rep5\"] = Ex_Auto_W17_30umTA_02ngQC_60m_3\n",
    "input_files[\"0.2ng_rep6\"] = Ex_Auto_W17_30umTA_02ngQC_60m_4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Parse Your Data Into A Dataframe. \n",
    "\n",
    "Below we are going to take your input files and get them into a data frames. Each dataframe that we make will be saved. \n",
    "\n",
    "Our filter function will drop any scans that have been flagged as decoys. We will also ensure that you do not have any duplicate scan numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No changes need to be made here. Please just run the cell. \n",
    "\n",
    "formatted_files = ml.filter_input(input_files, probability_column) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just look at one of your dfs/tables to make sure everying parsed correctly.\n",
    "You should have a scan and a peptide column as well as the column that you chose for the probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#No need to change anything, simply run the cell to view one of your dataframes.\n",
    "\n",
    "formatted_files[\"2ng_rep6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Make the MegaTables\n",
    "For each raw file, we will join your output with our output data into a megatable. This will allow you to see what your tools scans are finding relative to the tools that we benchamarked.\n",
    "To make the megascripts we will first read in our output for a specific raw file. We will then take our output and join your data to make the megascript. \n",
    "\n",
    "Each of the megascript will be saved as \"benchmark_MegaScript_\" + file name + \".csv\". The file name correlates to the name of the raw files. For example, if you want to look at all the data that came from the Ex_Auto_J3_30umTB_2ngQC_60m_1 file, we can view that megascript by reading in the \"benchmark_MegaScript_Ex_Auto_J3_30umTB_2ngQC_60m_1.csv\" file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#You do not need to input anything. Run the cell to make all your megascripts. \n",
    "\n",
    "ml.make_megascript(formatted_files, tool_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at one of our final megatables. You will see that there are 2 levels of heading for our megatable. We can see the probability and peptide that was found for each algorithm at a scan number. At scans where the algorithm did have an output for that scan number, the table will simply have \"Nan\". There will be spots where some tools do not have values and others did. \n",
    "\n",
    "Note: when saving a multiindexed dataframe to a .csv file, the format is not fully saved. We have to do some formatting to get it to look the way it was originally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting the megatable after it is read in from the csv.\n",
    "#Running this cell will take care of all the formatting; you do not need to change anything. \n",
    "\n",
    "df = pd.read_csv(\"benchmark_MegaScript_Ex_Auto_J3_30umTB_2ngQC_60m_1.csv\", header=[0,1])\n",
    "df = df.drop(columns = {\"Unnamed: 0_level_0\"})\n",
    "df = df.rename(columns={'Unnamed: 1_level_1': \" \"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Make the Graphs\n",
    "We will now make the graphs that will show us how many PSMs at or below our 1% FDR cutoff each tool is finding for the relative raw files. We will make one graph for all the 2ng files and another for all the 0.2ng files. \n",
    "\n",
    "The graphs will save to your computer as '2ng_native_score.png' and '0.2ng_native_score.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell makes the 2ng graph. Please just run this cell, there is no need to change anything.\n",
    "\n",
    "ml.make_2ng_graph(probability_column, tool_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell makes the 0.2ng graph. Please just run this cell, there is no need to change anything.\n",
    "\n",
    "ml.make_02ng_graph(probability_column, tool_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
